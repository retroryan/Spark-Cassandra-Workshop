#Intro to Spark Streaming and Cassandra

These exercises work through using Spark Streaming and Cassandra.

## Intro to Spark Streaming

Intro to Streaming from [Streaming Programing Guide](https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html)

Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Sparkâ€™s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.

![](https://spark.apache.org/docs/1.1.0/img/streaming-dstream.png)

Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.

![](https://spark.apache.org/docs/1.1.0/img/streaming-dstream-ops.png)

These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these details and provide the developer with higher-level API for convenience. These operations are discussed in detail in later sections.

##Exercise 1 - Listen to a socket to receive a basic text stream.  Perform a windowing word count on the text socket.

* The streaming exercises use two classes.  RunFeeder.java creates a basic word socket server to stream words to anything that opens a connection.  And RunReceiver.java is the base class for building the Spark Streaming applcation.

* To start the word socket server run `./runWordServer.sh`

* This starts the word socket server defined in `simpleStreaming.RunFeeder`

* Build and run the Spark Job with:

`dse spark-submit --class simpleStreaming.RunReceiver ./target/IntroSparkCassandra-0.1.jar 127.0.0.1 9999`

* The  for this exercise is found in `simpleStreaming.RunReceiver`

* A Spark Stream is built using a Streaming Context. The Streaming Context will divide the stream of data into micro-batches based on the batch Duration.  

* The Streaming Sample already creates a Streaming Context with a batch duration of 1 second:
```
new StreamingContext(SparkConfSetup.getSparkConf(), batchDuration);
```

* A receiver is then created from the streaming context.  The receiver can be any input stream that receives data over the network.  Each time it receives data from the network it stores the data into to Spark's memory.  These single items will be aggregated together into data blocks before being pushed into the next micro-batch for processing.

* Create a socket text stream that listens for a stream of words.  The storage level specifies where and how the received data gets stored before being processed:

```
javaStreamingContext.socketTextStream(
                "127.0.0.1", Integer.parseInt("9999"), StorageLevels.MEMORY_AND_DISK_SER);
```

* Similar to the previous word count, create a word count of the stream of words as they are received.  

* Use JavaDStream print to print out a debug string:

    `wordCountStream.print`

* Windowed operation operate on sliding window of data, for example if the batch is receiving data every 5 seconds then the following image would show working on the last 15 seconds of data:

![](https://spark.apache.org/docs/latest/img/streaming-dstream-window.png)

* Instead of keeping a count of every batch of words received instead  create a word count of the words received in the past 30 seconds every 10 seconds using `reduceByKeyAndWindow`

* How would you data model this in Cassandra?

* Save the word count to Cassandra.
